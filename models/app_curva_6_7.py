import streamlit as st

import pandas as pd
import io

# fichier des avis WebScraps de la Redoute
df = pd.read_csv("redoute.csv", sep=";")


# Titre principal
st.title("Customers Reviews Analytics")

# Barre lat√©rale avec le sommaire
st.sidebar.title("Sommaire")
pages = [
    "Introduction",
    "Collecte des Donn√©es",
    "Exploration et Analyse des Donn√©es",
    "Pr√©diction du Rating",
    "Analyse de Sentiment",
    "Topic Modeling",
    "Pr√©diction de la R√©ponse Fournisseur",
    "D√©mo",
    "Conclusion et Perspectives",
]

# S√©lection de la page dans la barre lat√©rale
page = st.sidebar.radio("Aller vers", pages)

# Contenu de chaque page
if page == pages[0]:
    st.write("## Introduction")

    st.write("#### Objectifs du Projet")
    st.markdown("- Pr√©dire les notations des clients")
    st.markdown("- Cat√©goriser les commentaires")
    st.markdown("- Proposer des r√©ponses automatiques")
    st.markdown("- Analyser les sentiments")

    st.write("#### Equipe Projet")
    st.text(
        """
        xxx
        xxx
        xxx
        """
    )

if page == pages[1]:
    st.write("## Collecte des Donn√©es")
    st.markdown("- Source de donn√©es: TrustedShops")
    st.markdown("- Entreprise cibl√©e : La Redoute")
    st.markdown("- WebScraping avec BeautifulSoup")
    st.markdown("- Donn√©es pour l'analyse exploratoire")

if page == pages[2]:
    st.write("## Exploration et Analyse des Donn√©es")
    st.markdown("- xx")
    st.markdown("- xx")
    st.markdown("- xx")

if page == pages[3]:
    st.write("## Pr√©diction du Rating en Fonction des Commentaires")
    st.write("#### Probl√©matique")
    # poser la probl√©matique

    st.write("#### D√©marche")
    # Mod√©lisation de base avec 3 variables explicatives
    # Bag of Words
    # GRU
    st.write("#### Mod√©lisation de Base")
    # Mod√©lisation de base avec 3 variables explicatives

    st.write("#### Bag of Words")
    # BoW , Tfidf , Cvtz ...

    st.write("#### GRU")
    # BoW , Tfidf , Cvtz ...

    st.write("#### R√©sultats")

if page == pages[4]:
    st.write("## Analyse de Sentiment")
    # Librairies pr√©-entra√Æn√©es : Vader, TextBlob, Bert
    # M√©canisme : Un texte ü°∫ un score de tonalit√© n√©gatif /positif
    # PieChart : Distribution du score de sentiments
    # Diagramme Barre => neutralit√© g√©n√©rale
    # Graphique : Neutralit√© des r√©ponses fournisseurs
    # Graphique : distribution rating versus Tonalit√©s sentiments

if page == pages[5]:
    st.write("## Topic Modeling")
    # objectifs : topics commentaires
    # D√©marche / LDA
    # R√©sultats

if page == pages[6]:
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    import scikit_posthocs as sp
    import statsmodels.api as sm
    import streamlit as st
    import pandas as pd
    import joblib
    import re
    import nltk
    import string
    import spacy
    import os


    from scipy.stats import levene
    from statsmodels.formula.api import ols
    from scipy.stats import kruskal
    from wordcloud import WordCloud
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize
    from nltk.sentiment import SentimentIntensityAnalyzer
    from sklearn.model_selection import train_test_split
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import accuracy_score
    from utils import nettoyer_texte
    from utils import extraire_noms
    from collections import Counter
    from sklearn.feature_extraction.text import CountVectorizer
    from nltk import bigrams, trigrams, FreqDist
    from sklearn.cluster import KMeans
    from textblob import TextBlob
    from transformers import pipeline
    from utils import sentiment_score_bert
    from utils import local_css

    nlp = spacy.load('fr_core_news_sm')
    nltk.download('punkt')
    nltk.download('stopwords')
    nltk.download('wordnet')
    nltk.download('vader_lexicon')


    # Fonction pour ins√©rer du CSS dans Streamlit
    def local_css(file_name):
        with open(file_name) as f:
            st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)

    # Ins√©rer le CSS personnalis√©
    local_css("style.css")


    st.write("# Pr√©diction de la R√©ponse Fournisseur")
    # Chargement des donn√©es
    st.text("")
    st.write('## Visualisation du DataFrame')
    df = pd.read_csv('redoute_v31.csv')

    st.text("")
    st.write(df.head())  # Affiche les premi√®res lignes du DataFrame
    st.text("")
    st.write('## EDA (Exploration de donn√©es)')
    st.text("")

    # 'createdAt' et 'SupplierReplyDate' 
    df['createdAt'] = pd.to_datetime(df['createdAt'])
    df['SupplierReplyDate'] = pd.to_datetime(df['SupplierReplyDate'])

    # Calculs n√©cessaires pour les graphiques
    df['Delay'] = (df['SupplierReplyDate'] - df['createdAt']).dt.days
    average_delay = df['Delay'].mean()
    ratings_per_month = df.groupby(df['createdAt'].dt.to_period("M"))['rating'].mean()
    supplier_replies_per_month = df.groupby(df['SupplierReplyDate'].dt.to_period("M")).size()

    # Choix de graphique dans la zone principale
    choix_graphique = st.selectbox(
    "S√©lectionnez le graphique √† afficher",
    ('Nombre de r√©ponses fournisseur par jour', 'Histogramme des d√©lais de r√©ponse', 'Analyse du d√©lai moyen par rating', '√âvaluation et r√©ponses fournisseur par mois' )
    )

    if choix_graphique == 'Nombre de r√©ponses fournisseur par jour':
        st.write("#### Nombre total de r√©ponses fournisseur par jour")
        fig, ax = plt.subplots()
        df.groupby(df['createdAt'].dt.date).size().plot(kind='line', ax=ax)
        plt.ylabel('Nombre de r√©ponses')
        plt.xticks(rotation=45)
        st.pyplot(fig)

    elif choix_graphique == 'Histogramme des d√©lais de r√©ponse':
        st.write("#### Histogramme des d√©lais de r√©ponse")
        fig, ax = plt.subplots()
        plt.hist(df['Delay'], bins=50, edgecolor="k", alpha=0.7)
        plt.axvline(average_delay, color='red', linestyle='dashed', linewidth=1, label=f'D√©lai moyen: {average_delay:.2f} jours')
        plt.title('Distribution des d√©lais de r√©ponse des fournisseurs')
        plt.xlabel('D√©lai (en jours)')
        plt.ylabel('Nombre de r√©ponses')
        plt.legend()
        st.pyplot(fig)

    elif choix_graphique == 'Analyse du d√©lai moyen par rating':
        st.write('#### Analyse du d√©lai moyen par rating')
        
        grouped = df.groupby('rating')['Delay'].mean().reset_index()
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.bar(grouped['rating'], grouped['Delay'], color=['red', 'orange', 'yellow', 'green', 'blue'])
        ax.set_title('D√©lai moyen par Rating')
        ax.set_xlabel('Rating')
        ax.set_ylabel('D√©lai moyen (en jours)')
        ax.set_xticks(grouped['rating'])
        ax.grid(axis='y')
        st.pyplot(fig)

    elif choix_graphique == '√âvaluation et r√©ponses fournisseur par mois':
        st.write("#### √âvaluation moyenne et nombre de r√©ponses fournisseur par mois")
        fig, ax1 = plt.subplots(figsize=(10, 6))

        color = 'tab:blue'
        ax1.set_xlabel('Mois')
        ax1.set_ylabel('√âvaluation moyenne', color=color)
        ax1.plot(ratings_per_month.index.astype('str'), ratings_per_month.values, color=color)
        ax1.tick_params(axis='y', labelcolor=color)

        ax2 = ax1.twinx()  

        color = 'tab:red'
        ax2.set_ylabel('Nombre de r√©ponses fournisseurs', color=color)  
        ax2.plot(supplier_replies_per_month.index.astype('str'), supplier_replies_per_month.values, color=color)
        ax2.tick_params(axis='y', labelcolor=color)

        fig.tight_layout()  
        plt.title('√âvaluation moyenne et nombre de r√©ponses fournisseurs par mois')
        st.pyplot(fig)

    
    st.text("")
    st.write("## Tests statistiques")
    st.text("")

    df = pd.read_csv('redoute_v31.csv')

    option_test = st.selectbox(
    "Choisissez le test statistique √† effectuer :",
    ("Homog√©n√©it√© des variances (Test de Levene)", "Normalit√© des r√©sidus (QQ-plot)", "Test de Kruskal-Wallis")
    )
    
    # 1/ Homog√©n√©it√© des variances / Test de Levene
    if option_test == "Homog√©n√©it√© des variances (Test de Levene)":
        st.write("#### 1. Test d'homog√©n√©it√© des variances (Test de Levene)")
    
        groups = [df['rating'][df['SupplierReply'] == reply] for reply in df['SupplierReply'].unique()]
        statistic, p_value = levene(*groups)
        st.write(f'Statistique de test: {statistic:.4f}')
        st.write(f'P-value: {p_value:.4g}')  # Affiche la p-value 

    # V√©rifie si la p-value est inf√©rieure √† votre alpha (par exemple, 0.05)
        if p_value < 0.05:
            st.write("Les variances ne sont pas √©gales selon le test de Levene.")
        else:
            st.write("Les variances sont √©gales selon le test de Levene.")
    
    # 2/ Normalit√© des r√©sidus
    elif option_test == "Normalit√© des r√©sidus (QQ-plot)":
        st.write("#### 2. Normalit√© des r√©sidus (Graphiquement avec un QQ-plot)")
    
    # Mod√®le OLS pour les r√©sidus
        model = ols('rating ~ C(SupplierReply)', data=df).fit()
        residus = model.resid
    
    # QQ-plot
        fig = sm.qqplot(residus, fit=True, line="45")
        st.pyplot(fig)

    # Test de Kruskal-Wallis
    elif option_test == "Test de Kruskal-Wallis":
        st.write("#### 3. Test de Kruskal-Wallis")

        groups = [group['rating'].values for name, group in df.groupby('SupplierReply')]
        stat, p_value = kruskal(*groups)

        st.write(f'Statistique de test: {stat}')
        st.write(f'P-value: {p_value}')

        alpha = 0.05
        if p_value < alpha:
            st.write("On rejette l'hypoth√®se nulle : il existe des diff√©rences significatives entre les groupes.")
        else:
            st.write("On ne peut pas rejeter l'hypoth√®se nulle : il n'y a pas de preuve de diff√©rences significatives entre les groupes.")

    # Fonction mise en cache
    @st.cache_data
    def calculer_test_dunn(notes, categories, methode):
        return sp.posthoc_dunn([notes[categories == k] for k in np.unique(categories)], p_adjust=methode)

    # Chargement des donn√©es
    df = pd.read_csv('redoute_v31.csv')
    notes = df['rating'].values
    categories = df['SupplierReply'].values

    # Titre 
    st.write("#### 4. Tests de Dunn avec diff√©rents ajustements")

    # S√©lection de la m√©thode d'ajustement
    option_ajustement = st.selectbox(
    "Choisissez la m√©thode d'ajustement pour le test de Dunn :",
    ("fdr_bh","bonferroni", "holm", )
    )

    

    # Calcul et affichage des r√©sultats
    if st.button("Effectuer le test de Dunn", key="unique_key_dunn_test"):
        p_values = calculer_test_dunn(notes, categories, option_ajustement)
        st.write(f"R√©sultats du test de Dunn avec ajustement {option_ajustement}:")

        # Cr√©ation de la heatmap de toutes les p-valeurs
        mask = np.triu(np.ones_like(p_values, dtype=bool))
        plt.figure(figsize=(10, 8))
        sns.heatmap(p_values, mask=mask, cmap='viridis', vmax=0.05)
        plt.title('Heatmap des p-valeurs ajust√©es')
        st.pyplot(plt)

        # Heatmap des comparaisons significatives
        alpha = 0.05
        significant_comparisons = p_values.where(p_values <= alpha)
        significant_mask = mask & (p_values <= alpha)
    
        plt.figure(figsize=(10, 8))
        sns.heatmap(significant_comparisons, mask=significant_mask, cmap='viridis', vmax=0.05, annot=True)
        plt.title('Comparaisons significatives (p <= 0.05)')
        st.pyplot(plt)

        # Affichage des paires significatives
        paires_significatives = np.where(p_values <= alpha)
        paires_indices = list(zip(paires_significatives[0], paires_significatives[1]))

    st.write("## Retour aux Verbatims")
    st.text("")
    st.markdown("""
    Notre analyse montre que les r√©ponses du service client diff√®rent selon le rating : 
    g√©n√©riques pour des notes √©lev√©es et personnalis√©es pour des notes basses, potentiellement due 
    √† la satisfaction ou √† des difficult√©s de contact. Les noms des agents peuvent influer sur l'√©valuation du sentiment, 
    ce qui nous pousse √† examiner de pr√®s les commentaires pour les ratings au-dessus et en dessous de 4.
    """, unsafe_allow_html=True)
    st.text("")
    st.write("## Analyse des R√©ponses pour les Ratings de 4 ou Plus")
    st.text("")
    st.write("#### **Approche WordCloud**")
    st.text("")
    # Importations n√©cessaires 
    # ... Votre code de pr√©traitement et de nettoyage ici ...

    # Appliquer la fonction de nettoyage √† la colonne 'SupplierReply'
    df_ratings_4_plus = df[df['rating'] >= 4]
    df_ratings_4_plus['SupplierReply_Cleaned'] = df_ratings_4_plus['SupplierReply'].apply(nettoyer_texte)
    # G√©n√©ration des diff√©rents WordClouds

    # Concat√©nation de tous les textes nettoy√©s dans une seule cha√Æne / Cr√©ation d'une fonction pour g√©n√©rer un wordcloud 
    texte_concatene = ' '.join(df_ratings_4_plus['SupplierReply_Cleaned'])
    
    def generate_wordcloud(stop_words):
        wordcloud = WordCloud(stopwords=stop_words, width=800, height=800, background_color='white').generate(texte_concatene)
        return wordcloud
    
    # D√©finition des diff√©rents ensembles de stop words
    stop_words_base = ['bonjour', 'merci', 'de', 'nous', 'vous', 'votre', 'pour', 'la', 'le', 'et', '√†', 'nos', 'des',
    'en', 'par', 'davoir', 'dans', 'un', 'une', 'sur', 'avec', 'cette', 'que', 'qui', 'plus',
    'sommes', 'notre', 'sont', '√™tre', 'ou', 'si', 'ils', 'les', 'comme', 'au', 'avoir', 'ce',
    'cet', 'cette', 'ces', 'mais', 'aussi', 'donc', 'lorsque', 'puis', 'car', 'tous', 'tout',
    'tr√®s', 'fait', 'faire', 'sans', 'chez', 'toujours', 'jamais', 'peut', 'peuvent', 'aussi',
    'client', 'service', '√©quipe', 'ilham', 'temps', 'partager', 'avis', 'exp√©rience', 'pris', 'prendre']

    stop_words_intermediaire = ['bonjour', 'merci', 'de', 'nous', 'vous', 'votre', 'pour', 'la', 'le', 'et', '√†', 'nos', 'des',
    'en', 'par', 'davoir', 'dans', 'un', 'une', 'sur', 'avec', 'cette', 'que', 'qui', 'plus',
    'sommes', 'notre', 'sont', '√™tre', 'ou', 'si', 'ils', 'les', 'comme', 'au', 'avoir', 'ce',
    'cet', 'cette', 'ces', 'mais', 'aussi', 'donc', 'lorsque', 'puis', 'car', 'tous', 'tout',
    'tr√®s', 'fait', 'faire', 'sans', 'chez', 'toujours', 'jamais', 'peut', 'peuvent', 'aussi',
    'client', 'service', '√©quipe', 'ilham', 'temps', 'partager', 'avis', 'exp√©rience', 'pris', 'prendre'
    '√†', 'de', 'du', 'la', 'le', 'nous', 'vos', 'votre', 'vous', 'et', 'a', 'des', 'en', 'les',
    'un', 'une', 'ont', '√™tre', 'est', 'pour', 'qui', 'que', 'dans', 'cette', 'vite', 'tout',
    'toute', 'plus', 'si', 'aussi', 'bien', 'comme', 'sans', 'sur', '√ßa', 'ont', 'disposition',
    't√©moigner', 'commentaire', 'remarques', 'fid√©lit√©', 'adresse', 'rubrique', 'remercions',
    'partages', 'site', 'loubna', 'wassim', 'jamila', 'zineb', 'encouragements', 'positif',
    'resterons', 'rester', 'satisfaites', 'satisfaire', 'heureux', 'plaisir', 'client', 'service',
    '√©quipe', 'ilham', 'temps', 'partager', 'avis', 'exp√©rience', 'pris', 'prendre']  # Ajoutez vos stop words interm√©diaires ici

    stop_words_avance = stop_words_intermediaire + ['footer', 'd√©sagr√©ment', '√™tes', 'satisfaite', 'voyons', 'partag√©es', 'bient√¥t',
    'aiden', 'resterons', 'contact', '√©quip', 'navi', 'gateur', 'navig', 'ateur',
    'compte', 'url', 'suivante', 'ravi', 'beaucoup', 'remercions', 'excusons',
    'lire', 'dautant', 'joindre', 'copiant', 'collant', 'am√©lior', '√©e', 'am√©liore',
    'souhaitons', 'souhaite', 'encore', 'adresse', 'contact', 'nous', 'r√©pondre',
    'rencontr√©s', 'invitons', 'souhaiterions', 'tent√©', 'appeler', 'pu', 'moteur',
    'recherche', 'revenir', 'vers', 'jamila', 'wassim', 'soukaina', 'zineb', 'enti√®re',
    'satisfaction', 'attendre', 'aide', 'prochain', 'prochaine', 'tente', 'souhaite',
    'navi', 'gateur', 'essayer', 'rester', 'aider', 'entier', 'enti√®re', 'satisfaire',
    'rester', 'enti√®re', 'enti√®rement', '√©cout', '√©quipe', '√©quipes', 'proposition',
    'proposer', 'vite', 'rapide', 'rapidement', 'prochainement', 'tenter', 'essayer',
    'atteindre', 'atteint', 'atteints', 'atteinte', 'atteintes', 'd√©√ßu', 'd√©√ßue',
    'd√©√ßus', 'd√©√ßues', 'heureux', 'heureuse', 'heureuses', 'content', 'contente',
    'contents', 'contentes', 'satisfait', 'satisfaits', 'satisfaite', 'satisfaites',
    'insatisfait', 'insatisfaite', 'insatisfaits', 'insatisfaites', 'excuse', 'excuses',
    'excusons', 'excuser', 'd√©sol√©', 'd√©sol√©e', 'd√©sol√©s', 'd√©sol√©es', 'regret', 'regrets',
    'regrette', 'regretter', 'regrett√©', 'regrett√©e', 'regrett√©s', 'regrett√©es', 'souci',
    'soucis', 'pr√©occupation', 'pr√©occupations', 'inqui√©tude', 'inqui√©tudes', 'probl√®me',
    'probl√®mes', 'probl√©matique', 'probl√©matiques', 'question', 'questions', 'demande',
    'demandes', 'requ√™te', 'requ√™tes', 'r√©ponse', 'r√©ponses', 'solution', 'solutions',
    'r√©solution', 'r√©solutions', 'solliciter', 'sollicit√©', 'sollicit√©e', 'sollicit√©s',
    'sollicit√©es', 'suggestion', 'suggestions', 'conseil', 'conseils', 'recommandation',
    'recommandations', 'avis', 'opinion', 'opinions', 'feedback', 'feedbacks', 'retour',
    'retours', 'critique', 'critiques', '√©valuation', '√©valuations', 'appr√©ciation',
    'appr√©ciations', 'commentaire', 'commentaires']  

    # Dictionnaire des wordclouds
    wordclouds = {
    'Base': generate_wordcloud(stop_words_base),
    'Interm√©diaire': generate_wordcloud(stop_words_intermediaire),
    'Avanc√©': generate_wordcloud(stop_words_avance),
    }

    # Widget pour s√©lectionner le wordcloud
    option = st.selectbox('Choisissez le niveau de wordcloud', ('Base', 'Interm√©diaire', 'Avanc√©'))

    # Affichage du wordcloud s√©lectionn√©
    st.write(f"WordCloud Niveau : {option}")
    wordcloud = wordclouds[option]
    fig, ax = plt.subplots(figsize=(8, 8))
    ax.imshow(wordcloud)
    ax.axis("off")
    st.pyplot(fig)
    st.text("")
    st.markdown("""
    La g√©n√©ration de ses WordCloud successifs apr√®s filtrage par des  stop words  donne du sens √† notre postulat de d√©part sur l'hypoth√®se d'un **caract√®re g√©n√©rique des r√©ponses**.
    Mais cela ne suffit pas pour justifier l'assertion. Nous avons donc proced√© √† d'autres test.
    """, unsafe_allow_html=True)
    st.text("")
    st.write("#### **Tests compl√©mentaires**")
    st.text("")

    # Cr√©ation des cases √† cocher pour chaque test
    test_freq_mots = st.checkbox('Analyse de Fr√©quence des Mots', value=False)
    test_co_occurrence = st.checkbox('Analyse de Co-occurrence', value=False)
    test_bigrammes_trigrammes = st.checkbox('Analyse des Bigrammes/Trigrammes', value=False)
    test_diversite_lexicale = st.checkbox('Diversit√© Lexicale', value=False)
    test_clustering_texte = st.checkbox('Classification et Clustering de Texte', value=False)
    test_sentiment_textblob = st.checkbox('Analyse de Sentiment simple "TextBlob"', value=False)

    # Condition pour afficher le r√©sultat de l'Analyse de Fr√©quence des Mots si la case est coch√©e
    if test_freq_mots:
        # Tokenisation du texte
        mots = texte_concatene.split()

        # Comptage de la fr√©quence de chaque mot
        frequence_mots = Counter(mots)

        # Convertion du compteur en DataFrame pour une meilleure lisibilit√©
        df_frequence_mots = pd.DataFrame(frequence_mots.items(), columns=['Mot', 'Fr√©quence']).sort_values(by='Fr√©quence', ascending=False)
    
        # Afficher les mots les plus fr√©quents dans Streamlit
        st.dataframe(df_frequence_mots.head(20))

        # Afficher un histogramme des mots les plus fr√©quents
        fig, ax = plt.subplots()
        ax.bar(df_frequence_mots['Mot'].head(20), df_frequence_mots['Fr√©quence'].head(20))
        ax.set_xticklabels(df_frequence_mots['Mot'].head(20), rotation=90)
        ax.set_title("Top 20 des mots les plus fr√©quents")
        ax.set_xlabel("Mots")
        ax.set_ylabel("Fr√©quence")

        # Utiliser st.pyplot() pour afficher la figure
        st.pyplot(fig)

        st.write("R√©sultat de l'Analyse de Fr√©quence des Mots")

    # Condition pour afficher le r√©sultat de l'Analyse de Co-occurrence si la case est coch√©e
    if test_co_occurrence:
        # Utiliser CountVectorizer pour cr√©er une matrice de termes
        vectorizer = CountVectorizer()
        X = vectorizer.fit_transform(df_ratings_4_plus['SupplierReply_Cleaned'])

        # Conversion de la matrice de termes en array numpy
        Xc = (X.T * X)  # Ceci est la matrice de co-occurrence
        Xc.setdiag(0)  # Remplacer la diagonale par 0s

        # Cr√©er un DataFrame √† partir de la matrice de co-occurrence
        noms_mots = vectorizer.get_feature_names_out()
        df_co_occurrence = pd.DataFrame(data=Xc.toarray(), index=noms_mots, columns=noms_mots)

        # Cr√©er un heatmap avec seaborn
        fig, ax = plt.subplots(figsize=(10, 10))
        sns.heatmap(df_co_occurrence.iloc[:20, :20], ax=ax)

        # Afficher le heatmap
        st.pyplot(fig)
        # Afficher la matrice de co-occurrence pour les 20 premiers mots
        st.dataframe(df_co_occurrence.iloc[:20, :20])

        st.write("R√©sultat de l'Analyse de Co-occurrence")

        

    # Condition pour afficher le r√©sultat de l'Analyse des Bigrammes/Trigrammes si la case est coch√©e
    if test_bigrammes_trigrammes:
        # Tokenisation du texte
        mots = texte_concatene.split()

        # Cr√©er des listes de bigrammes et trigrammes
        bigramme_liste = list(bigrams(mots))
        trigramme_liste = list(trigrams(mots))

        # Calculer la fr√©quence des bigrammes et trigrammes
        bigramme_freq = FreqDist(bigramme_liste)
        trigramme_freq = FreqDist(trigramme_liste)

        # Convertir les fr√©quences en DataFrames
        df_bigrammes = pd.DataFrame(bigramme_freq.most_common(20), columns=['Bigramme', 'Fr√©quence'])
        df_trigrammes = pd.DataFrame(trigramme_freq.most_common(20), columns=['Trigramme', 'Fr√©quence'])

        # Afficher les DataFrames
        st.write("Bigrammes les plus courants :")
        st.table(df_bigrammes)

        st.write("Trigrammes les plus courants :")
        st.table(df_trigrammes)

    # Condition pour afficher le r√©sultat de la Diversit√© Lexicale si la case est coch√©e
    if test_diversite_lexicale:
        # Tokenisation du texte
        mots = texte_concatene.split()
        # Nombre total de mots
        total_mots = len(mots)

        # Nombre de mots uniques
        mots_uniques = len(set(mots))

        # Calcul de la diversit√© lexicale
        diversite_lexicale = mots_uniques / total_mots

        # Affichage de la diversit√© lexicale dans l'application
        st.write(f"Diversit√© Lexicale: {diversite_lexicale:.2f}")

    # Condition pour afficher le r√©sultat de la Classification et Clustering de Texte si la case est coch√©e
    if test_clustering_texte:
        # Transformer les donn√©es textuelles en TF-IDF
        tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words_avance)
        tfidf = tfidf_vectorizer.fit_transform(df_ratings_4_plus['SupplierReply_Cleaned'])

        # D√©finir le mod√®le K-Means
        kmeans = KMeans(n_clusters=5, random_state=42)
        kmeans.fit(tfidf)

        # Afficher les centres de cluster
        order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
        terms = tfidf_vectorizer.get_feature_names_out()

        # Affichage des mots de chaque cluster dans l'application
        st.write("### R√©sultats de la Classification par K-Means")
        for i in range(5):
            top_ten_words = [terms[ind] for ind in order_centroids[i, :10]]
            cluster_words = ", ".join(top_ten_words)
            st.write(f"Cluster {i}: {cluster_words}")


    # Condition pour afficher le r√©sultat de l'Analyse de Sentiment simple "TextBlob" si la case est coch√©e
    if test_sentiment_textblob:
        # Ajouter une colonne pour le sentiment
        df_ratings_4_plus['Polarity'] = df_ratings_4_plus['SupplierReply_Cleaned'].apply(lambda text: TextBlob(text).sentiment.polarity)
    
        # Visualiser la distribution du sentiment
        st.write("### R√©sultat de l'Analyse de Sentiment simple 'TextBlob'")
        fig, ax = plt.subplots()
        sns.histplot(df_ratings_4_plus['Polarity'], bins=30, kde=False, ax=ax)
        ax.set_title('Distribution de Polarit√© des Sentiments')
        ax.set_xlabel('Polarit√©')
        ax.set_ylabel('Fr√©quence')
        st.pyplot(fig)

    st.text("")
    st.write("## Analyse des R√©ponses pour les Ratings inferieur ou √©gale √† 3")
    st.text("")
    

    # D√©finir les options de la selectbox

    options_analyse = {
    "Analyse de Contenu": None,
    "Identification du Personnel R√©pondant": extraire_noms,
    "S√©paration des r√©ponses d'Ilham et cumul par intervenant": None  # Pas de fonction associ√©e pour l'instant
    }

    # Widget pour s√©lectionner l'analyse
    option_selectionnee = st.selectbox(
    "Choisissez l'analyse √† afficher :",
    options=list(options_analyse.keys())
    )
    


    # Appliquer l'analyse s√©lectionn√©e
    if option_selectionnee == "Analyse de Contenu":
        df_ratings_1_3 = df[df['rating'] <= 3]
        df_ratings_1_3['SupplierReply_Cleaned'] = df_ratings_1_3['SupplierReply'].apply(nettoyer_texte)
    
        # D√©finition de stop_words
        stop_words = [
        'bonjour', 'merci', 'de', 'nous', 'vous', 'votre', 'pour', 'la', 'le', 'et', '√†', 'nos', 'des',
        'en', 'par', 'davoir', 'dans', 'un', 'une', 'sur', 'avec', 'cette', 'que', 'qui', 'plus',
        'sommes', 'notre', 'sont', '√™tre', 'ou', 'si', 'ils', 'les', 'comme', 'au', 'avoir', 'ce',
        'cet', 'cette', 'ces', 'mais', 'aussi', 'donc', 'lorsque', 'puis', 'car', 'tous', 'tout',
        'tr√®s', 'fait', 'faire', 'sans', 'chez', 'toujours', 'jamais', 'peut', 'peuvent', 'aussi',
        'client', 'service', '√©quipe', 'ilham', 'temps', 'partager', 'avis', 'exp√©rience', 'pris', 'prendre'
        '√†', 'de', 'du', 'la', 'le', 'nous', 'vos', 'votre', 'vous', 'et', 'a', 'des', 'en', 'les',
        'un', 'une', 'ont', '√™tre', 'est', 'pour', 'qui', 'que', 'dans', 'cette', 'vite', 'tout',
        'toute', 'plus', 'si', 'aussi', 'bien', 'comme', 'sans', 'sur', '√ßa', 'ont', 'disposition',
        't√©moigner', 'commentaire', 'remarques', 'fid√©lit√©', 'adresse', 'rubrique', 'remercions',
        'partages', 'site', 'loubna', 'wassim', 'jamila', 'zineb', 'encouragements', 'positif',
        'resterons', 'rester', 'satisfaites', 'satisfaire', 'heureux', 'plaisir', 'client', 'service',
        '√©quipe', 'ilham', 'temps', 'partager', 'avis', 'exp√©rience', 'pris', 'prendre', 'footer', 'd√©sagr√©ment', '√™tes', 'satisfaite', 'voyons', 'partag√©es', 'bient√¥t',
        'aiden', 'resterons', 'contact', '√©quip', 'navi', 'gateur', 'navig', 'ateur',
        'compte', 'url', 'suivante', 'ravi', 'beaucoup', 'remercions', 'excusons',
        'lire', 'dautant', 'joindre', 'copiant', 'collant', 'am√©lior', '√©e', 'am√©liore',
        'souhaitons', 'souhaite', 'encore', 'adresse', 'contact', 'nous', 'r√©pondre',
        'rencontr√©s', 'invitons', 'souhaiterions', 'tent√©', 'appeler', 'pu', 'moteur',
        'recherche', 'revenir', 'vers', 'jamila', 'wassim', 'soukaina', 'zineb', 'enti√®re',
        'satisfaction', 'attendre', 'aide', 'prochain', 'prochaine', 'tente', 'souhaite',
        'navi', 'gateur', 'essayer', 'rester', 'aider', 'entier', 'enti√®re', 'satisfaire',
        'rester', 'enti√®re', 'enti√®rement', '√©cout', '√©quipe', '√©quipes', 'proposition',
        'proposer', 'vite', 'rapide', 'rapidement', 'prochainement', 'tenter', 'essayer',
        'atteindre', 'atteint', 'atteints', 'atteinte', 'atteintes', 'd√©√ßu', 'd√©√ßue',
        'd√©√ßus', 'd√©√ßues', 'heureux', 'heureuse', 'heureuses', 'content', 'contente',
        'contents', 'contentes', 'satisfait', 'satisfaits', 'satisfaite', 'satisfaites',
        'insatisfait', 'insatisfaite', 'insatisfaits', 'insatisfaites', 'excuse', 'excuses',
        'excusons', 'excuser', 'd√©sol√©', 'd√©sol√©e', 'd√©sol√©s', 'd√©sol√©es', 'regret', 'regrets',
        'regrette', 'regretter', 'regrett√©', 'regrett√©e', 'regrett√©s', 'regrett√©es', 'souci',
        'soucis', 'pr√©occupation', 'pr√©occupations', 'inqui√©tude', 'inqui√©tudes', 'probl√®me',
        'probl√®mes', 'probl√©matique', 'probl√©matiques', 'question', 'questions', 'demande',
        'demandes', 'requ√™te', 'requ√™tes', 'r√©ponse', 'r√©ponses', 'solution', 'solutions',
        'r√©solution', 'r√©solutions', 'solliciter', 'sollicit√©', 'sollicit√©e', 'sollicit√©s',
        'sollicit√©es', 'suggestion', 'suggestions', 'conseil', 'conseils', 'recommandation',
        'recommandations', 'avis', 'opinion', 'opinions', 'feedback', 'feedbacks', 'retour',
        'retours', 'critique', 'critiques', '√©valuation', '√©valuations', 'appr√©ciation',
        'appr√©ciations', 'commentaire', 'commentaires'
        ] 

        # Initialisation du Vectorizer
        vectorizer = CountVectorizer(stop_words=stop_words)

        # Application du vectorizer aux r√©ponses
        X = vectorizer.fit_transform(df_ratings_1_3['SupplierReply_Cleaned'])

        # Conversion en DataFrame pour une meilleure lisibilit√©
        df_mots_freq = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())

        # Affichage des termes les plus fr√©quents
        df_mots_freq_sum = df_mots_freq.sum().sort_values(ascending=False).head(20)
    
        # Utilisation de Streamlit pour afficher les r√©sultats
        st.write("R√©sultats de l'Analyse de Contenu")
        st.dataframe(df_mots_freq_sum)
        pass 

    elif option_selectionnee == "Identification du Personnel R√©pondant":

        df_ratings_1_3 = df[df['rating'] <= 3].copy()
        df_ratings_1_3['Noms'] = df_ratings_1_3['SupplierReply'].apply(extraire_noms)

        # Afficher les r√©sultats dans Streamlit
        st.write("R√©sultats de l'Identification du Personnel R√©pondant :")
        st.dataframe(df_ratings_1_3[['SupplierReply', 'Noms']].head(15))
        
        
    elif option_selectionnee == "S√©paration des r√©ponses d'Ilham et cumul par intervenant":

        df_ratings_1_3 = df[df['rating'] <= 3].copy()
        df_ratings_1_3['Noms'] = df_ratings_1_3['SupplierReply'].apply(extraire_noms)

        # S√©parer les r√©ponses d'Ilham
        df_ilham = df_ratings_1_3[df_ratings_1_3['Noms'].apply(lambda x: 'Ilham' in x)]

        # S√©parer les r√©ponses des autres intervenants
        df_autres = df_ratings_1_3[df_ratings_1_3['Noms'].apply(lambda x: 'Ilham' not in x)]

        # Faire le cumul des r√©ponses par intervenant
        cumul_intervenants = df_ratings_1_3.explode('Noms')['Noms'].value_counts()

        # Afficher les r√©sultats dans Streamlit
        st.write("R√©ponses d'Ilham :")
        st.dataframe(df_ilham)

        st.write("R√©ponses des autres intervenants :")
        st.dataframe(df_autres)

        st.write("Cumul des r√©ponses par intervenant :")
        st.dataframe(cumul_intervenants.reset_index().rename(columns={'index': 'Intervenant', 'Noms': 'Nombre de r√©ponses'}))

    

# Chargement et pr√©paration des donn√©es
    @st.cache_data
    def charger_donnees(file):
        return pd.read_csv(file, sep=",")

    @st.cache_data
    def preprocess_text(text):
    # Cr√©ation du stemmer √† l'int√©rieur de la fonction
        stemmer = nltk.SnowballStemmer("french")
        text = text.lower()
        text = re.sub(r'http\S+', '', text)
        text = re.sub(r'bonjour,','', text)
        text = re.sub(r'[\w\.-]+ de l\'√©quipe service client', '', text)
        text = re.sub(r'[^\w\s]', '', text)
        text = re.sub(r'\d+', '', text)
        tokens = word_tokenize(text)
        tokens = [word for word in tokens if word not in stopwords.words('french')]
        tokens = [stemmer.stem(word) for word in tokens]
        return ' '.join(tokens)

    @st.cache_data
    def entrainer_modele_naif(_X_train, _y_train):
        model = LogisticRegression(max_iter=1000)
        model.fit(_X_train, _y_train)
        return model

    @st.cache_data
    def analyser_sentiments(text):
        sia = SentimentIntensityAnalyzer()
        return sia.polarity_scores(text)

    # Chargez les donn√©es
    df = charger_donnees('redoute_v31.csv')

    # Streamlit UI
    st.text("")
    st.title("Analyse Sentiments / Mod√©lisation VADER, Multi-Classes")
    st.text("")

    # Mod√©lisation Na√Øve
    st.text("")
    st.header("Mod√©lisation Na√Øve")
    st.text("")
    df['processed_comments'] = df['SupplierReply'].apply(preprocess_text)
    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(df['processed_comments'])
    y = df['rating'].values
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model_naif = entrainer_modele_naif(X_train, y_train)
    y_pred = model_naif.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    st.write(f"Pr√©cision du mod√®le na√Øf: {accuracy:.2f}")
    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(df['processed_comments'])
    joblib.dump(vectorizer, 'vectorizer.joblib')
    

    # Mod√®le VADER
    st.text("")
    st.header("Mod√®le VADER")
    st.text("")

    df['sentiment_scores'] = df['SupplierReply'].apply(lambda x: analyser_sentiments(x)['compound'])
    df[['id','SupplierReply', 'sentiment_scores']].to_csv('sentiments_vader.csv', index=False)


    # Modele BERT
    
    # Modele BERT
    st.text("")
    st.text("")
    st.header("Modele BERT")
    st.text("")

    # Chemin du fichier CSV
    chemin_csv = 'sentiments_bert.csv'

    # V√©rifier si le fichier CSV existe d√©j√†
    if os.path.isfile(chemin_csv):
        # Charger les scores de sentiment BERT directement depuis le fichier CSV
        df_sentiment_bert = pd.read_csv(chemin_csv)
    else:
        # Charger le pipeline 'sentiment-analysis' avec le mod√®le multilingue
        nlp_sentiment = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')
    
        # Fonction pour obtenir le score de sentiment avec BERT
        def sentiment_score_bert(texte):
            try:
                result = nlp_sentiment(texte[:512])[0]  # Tronquer le texte √† 512 tokens si n√©cessaire
                return {'label': result['label'], 'score': result['score']}
            except Exception as e:
                return {'label': 'NEUTRAL', 'score': 0.0}

        # Appliquer la fonction aux r√©ponses pour calculer les scores de sentiment BERT
        df['sentiment_bert'] = df['SupplierReply'].apply(sentiment_score_bert)

        # Sauvegarder les r√©sultats dans un fichier CSV
        df[['id','SupplierReply', 'sentiment_bert']].to_csv(chemin_csv, index=False)

    st.write("Les mod√®les ont √©t√© entra√Æn√©s et les r√©sultats sont sauvegard√©s pour la d√©monstration.")
    
    st.text("")
    st.text("")
    st.text("")

    st.header("**Synthese:**")
    st.text("")
    st.markdown("""
                
    Les r√©ponses des fournisseurs aux avis de 1 √† 3 √©toiles sont typiques et semi-personnalis√©es, 
    contrastant avec les r√©ponses plus positives aux avis de 4 √©toiles et plus. Cette uniformit√©, 
    m√™me dans les tentatives de personnalisation comme l'ajout de noms d'employ√©s, indique un script 
    standardis√© pour maintenir la constance du service. La standardisation des r√©ponses, √©vidente dans 
    l'analyse des 'topics', sugg√®re la possibilit√© d'automatiser les r√©ponses par labels, bien que la d√©tection 
    de sentiment dans les r√©ponses actuelles ne soit pas un indicateur fiable du rating.

    """, unsafe_allow_html=True)

if page == pages[7]:
    import streamlit as st
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    import nltk
    from nltk.sentiment import SentimentIntensityAnalyzer
    from sklearn.model_selection import train_test_split
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import accuracy_score
    import re
    import joblib
    from nltk.stem.snowball import SnowballStemmer
    from utils import preprocess_text
    from utils import analyser_sentiments
    from utils import sentiment_score_bert
    from utils import local_css

    # Fonction pour ins√©rer du CSS dans Streamlit
    def local_css(file_name):
        with open(file_name) as f:
            st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)

    # Ins√©rer le CSS personnalis√©
    local_css("style.css")

    st.text("")
    st.text("")
    # commentaires pos, neg, ambigu
    # pr√©diction du rating
    # pr√©diction du sentiment
    # pr√©diction du topic
    # pr√©diction r√©ponse fournisseur
    st.title("D√©monstration des Mod√®les")
    st.text("")
    st.text("")
    # Chargement du mod√®le pr√©-entra√Æn√© et du vectoriseur
    model_naif = joblib.load('modele_naif.joblib')
      
    

    # Assurez-vous que le chemin est correct et que 'vectorizer.joblib' existe dans ce chemin
    vectorizer = joblib.load('vectorizer.joblib')
    # Option pour s√©lectionner le mod√®le √† d√©montrer
    modele_option = st.selectbox("Choisir le mod√®le √† d√©montrer:", ["Mod√®le Na√Øf", "Mod√®le VADER","Mod√®le BERT"])

    if modele_option == "Mod√®le Na√Øf":
        # UI pour entr√©e utilisateur pour le mod√®le Na√Øf
        user_input = st.text_area("Entrez un texte pour classification par le mod√®le Na√Øf:", "Ce produit est pas mal !")
        if st.button("Classer avec le mod√®le Na√Øf"):
            processed_input = preprocess_text(user_input)  
            prediction = model_naif.predict(vectorizer.transform([processed_input]))
            st.write(f"Pr√©diction de la classe : {prediction[0]}")
 

    elif modele_option == "Mod√®le VADER":
        df_sentiments = pd.read_csv('sentiments_vader.csv')
        user_selection = st.radio("Choisir un exemple ou entrer un nouveau texte pour VADER", ['Choisir un exemple', 'Entrer un texte'])

        if user_selection == 'Choisir un exemple':
            example_id = st.selectbox("Choisir un ID de commentaire pour VADER:", df_sentiments['id'].values)
            selected_comment = df_sentiments.loc[df_sentiments['id'] == example_id, 'SupplierReply'].iloc[0]
            st.write(f"Commentaire pour l'ID {example_id}: {selected_comment}")
            selected_score = df_sentiments.loc[df_sentiments['id'] == example_id, 'sentiment_scores'].iloc[0]
            st.write(f"Scores de sentiment pour l'ID {example_id}: {selected_score}")
        else:
            user_input = st.text_area("Entrez un texte pour analyse de sentiment par VADER:", "Ce produit est excellent !")
            if st.button("Analyser Sentiment avec VADER"):
                sentiment_scores = analyser_sentiments(user_input)
                st.write(sentiment_scores)
    
    
    elif modele_option == "Mod√®le BERT":
        df_sentiments_bert = pd.read_csv('sentiments_bert.csv')  # Charger les scores de sentiment BERT
        user_selection = st.radio("Choisir un exemple ou entrer un nouveau texte pour BERT", ['Choisir un exemple', 'Entrer un texte'])

        if user_selection == 'Choisir un exemple':
            example_id = st.selectbox("Choisir un ID de commentaire pour BERT:", df_sentiments_bert['id'].values)
            selected_comment = df_sentiments_bert.loc[df_sentiments_bert['id'] == example_id, 'SupplierReply'].iloc[0]
            st.write(f"Commentaire pour l'ID {example_id}: {selected_comment}")
            selected_score = df_sentiments_bert.loc[df_sentiments_bert['id'] == example_id, 'sentiment_bert'].iloc[0]
            st.write(f"Scores de sentiment BERT pour l'ID {example_id}: {selected_score}")
        else:
            user_input = st.text_area("Entrez un texte pour analyse de sentiment par BERT ou choisissez un exemple ci-dessous:", "J'ai command√© un colis mais il n'est jamais arriv√©.")

            exemples_textes = [
                "J'ai command√© un colis mais il n'est jamais arriv√©.",
                "La livraison a √©t√© retard√©e sans aucune notification pr√©alable.",
                "Mon paquet est arriv√© endommag√©, tr√®s d√©√ßu du service.",
                "Le suivi de mon colis indique qu'il a √©t√© livr√© mais je ne l'ai pas re√ßu.",
                "Tr√®s m√©content, mon produit est arriv√© trois semaines en retard.",
                "Le service client ne m'a pas aid√© √† r√©soudre mon probl√®me de livraison.",
                "J'ai re√ßu le mauvais produit et je dois maintenant attendre le retour.",
                "Commande perdue par le transporteur, et maintenant je dois attendre un remboursement.",
                "L'article livr√© ne correspond pas √† la description du site, probl√®mes de retour.",
                "Pas de communication de la part du transporteur, je ne sais pas o√π est mon colis."
            ]

            exemple_selectionne = st.selectbox("Ou s√©lectionnez un exemple de probl√®me de livraison :", exemples_textes)

            if st.button("Analyser Sentiment avec BERT"):
                # S'il y a une saisie de l'utilisateur, utilisez-la, sinon utilisez l'exemple s√©lectionn√©
                texte_a_analyser = user_input if user_input else exemple_selectionne
                sentiment_result = sentiment_score_bert(texte_a_analyser)
                st.write(sentiment_result)







    # elif modele_option == "Mod√®le BERT":
    #     df_sentiments_bert = pd.read_csv('sentiments_bert.csv')  # Charger les scores de sentiment BERT
    #     user_selection = st.radio("Choisir un exemple ou entrer un nouveau texte pour BERT", ['Choisir un exemple', 'Entrer un texte'])

    #     if user_selection == 'Choisir un exemple':
    #         example_id = st.selectbox("Choisir un ID de commentaire pour BERT:", df_sentiments_bert['id'].values)
    #         selected_comment = df_sentiments_bert.loc[df_sentiments_bert['id'] == example_id, 'SupplierReply'].iloc[0]
    #         st.write(f"Commentaire pour l'ID {example_id}: {selected_comment}")
    #         selected_score = df_sentiments_bert.loc[df_sentiments_bert['id'] == example_id, 'sentiment_bert'].iloc[0]
    #         st.write(f"Scores de sentiment BERT pour l'ID {example_id}: {selected_score}")
    #     else:
    #         user_input = st.text_area("Entrez un texte pour analyse de sentiment par BERT:", "Ce produit est excellent !")
    #         if st.button("Analyser Sentiment avec BERT"):
    #             sentiment_result = sentiment_score_bert(user_input)
    #             st.write(sentiment_result)



if page == pages[8]:
    st.write("## Conclusion & Perspectives")
    # Conclusions
    # Perspectives
